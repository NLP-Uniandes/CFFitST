{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72a2ca4-36e6-4702-841c-3ae00802be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afbenitez/NLSBE/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from CFFit import CFFitST, ClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ffe364-3684-4b13-9a0d-ddef455e78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"all-mpnet-base-v2\"\n",
    "RANDOM_SEED = 23\n",
    "OUTPUT_PATH = 'output'\n",
    "\n",
    "import torch\n",
    "\n",
    "device_num = 3\n",
    "DEVICE = \"cuda:\"+str(device_num)\n",
    "torch.cuda.set_device(device_num)\n",
    "DEVICE = DEVICE if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8a074e-1c17-4ffc-a6a8-2ae991d212a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"data/issues_train.csv\")\n",
    "test_set = pd.read_csv(\"data/issues_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307fa414-42d2-46f6-825b-279e328e20f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['microsoft/vscode', 'tensorflow/tensorflow', 'facebook/react', 'opencv/opencv', 'bitcoin/bitcoin']\n"
     ]
    }
   ],
   "source": [
    "repos = list(set(train_set[\"repo\"].unique()))\n",
    "print(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e6e24e-d111-4b96-ba59-cc20e7886d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>bug</th>\n",
       "      <th>feature</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>repo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bitcoin/bitcoin</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/react</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/vscode</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opencv/opencv</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tensorflow/tensorflow</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label                  bug  feature  question\n",
       "repo                                         \n",
       "bitcoin/bitcoin        100      100       100\n",
       "facebook/react         100      100       100\n",
       "microsoft/vscode       100      100       100\n",
       "opencv/opencv          100      100       100\n",
       "tensorflow/tensorflow  100      100       100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.groupby([\"repo\", \"label\"]).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b90714e6-b008-41e4-bf42-1c325eb32364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 17:56:37.071630: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-06 17:56:37.112267: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-06 17:56:37.112297: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-06 17:56:37.113309: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-06 17:56:37.119894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-06 17:56:37.883060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def process_dataset(df):\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #print(row)\n",
    "        df.at[i,'text'] = str(row['title']) + \" \" + str(row['body'])\n",
    "    df = df[['text', 'label', 'repo']]\n",
    "    return df\n",
    "    \n",
    "train_set, test_set = process_dataset(train_set), process_dataset(test_set)\n",
    "\n",
    "dic_labels = {\"feature\":0,\"bug\":1,\"question\":2}\n",
    "def get_labels(data_set):\n",
    "    labels = data_set[\"label\"]\n",
    "    return to_categorical([ dic_labels[label] for i, label in labels.items()], num_classes=3)\n",
    "\n",
    "\n",
    "\n",
    "def get_x_y(df):\n",
    "    x = df[\"text\"].to_list()\n",
    "    y = get_labels(df)\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c940ba91-06d3-4038-a90e-d943db32eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "dic_labels = {\"feature\":0,\"bug\":1,\"question\":2}\n",
    "def class_report(y_true, y_pred, name_repo):\n",
    "  \n",
    "  # Convert the predicted probabilities to class labels\n",
    "  y_pred_classes = np.argmax(y_pred, axis=1)  # Assuming a one-hot encoded target variable\n",
    "\n",
    "  # Convert the true labels to class labels (if needed)\n",
    "  y_true_classes = np.argmax(y_true, axis=1)  # Replace 'y_true' with your true labels\n",
    "\n",
    "  # Generate the classification report\n",
    "  report = classification_report(y_true_classes, y_pred_classes)\n",
    "  print(name_repo)\n",
    "  print(report)\n",
    "  # Calcular la matriz de confusión\n",
    "  matriz_confusion = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "  # Crear un heatmap de la matriz de confusión\n",
    "  plt.figure(figsize=(4, 3))\n",
    "  sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues',\n",
    "              xticklabels=list(dic_labels.keys()), yticklabels=list(dic_labels.keys()))\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.ylabel('Actual')\n",
    "  plt.title(name_repo)\n",
    "  plt.show()\n",
    "  return classification_report(y_true_classes, y_pred_classes, output_dict=True, digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179aae76-0d87-4788-82ec-44e4b0973c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': {'save_path': 'output', 'labels': ['bug', 'feature', 'question'], 'epochs': 3, 'validation_data': 0.1, 'chunk_size': 0.25, 'positive_threshold': 0.95, 'negative_threshold': 0.05, 'chunks_reviewed': 3, 'batch_size': 32, 'min_chunk_size': 0.2}}\n",
      "Iteration 0\n",
      "bug feature st_pos 0 size 4500.0 end_pos 4500.0\n",
      "bug question st_pos 0 size 4500.0 end_pos 4500.0\n",
      "feature question st_pos 0 size 4500.0 end_pos 4500.0\n",
      "bug bug st_pos 0 size 4455.0 end_pos 4455.0\n",
      "feature feature st_pos 0 size 4455.0 end_pos 4455.0\n",
      "question question st_pos 0 size 4455.0 end_pos 4455.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n",
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n",
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n",
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n",
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n",
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate data  ('bug', 'feature') l 0 s: 1000 \n",
      "[==========] 1000 1000 1.0\n",
      "Validate data  ('bug', 'question') l 0 s: 1000 \n",
      "[==========] 1000 1000 1.0\n",
      "Validate data  ('feature', 'question') l 0 s: 1000 \n",
      "[==========] 1000 1000 1.0\n",
      "Validate data  ('bug', 'bug') l 1 s: 990 \n",
      "[==========] 990 990 1.0\n",
      "Validate data  ('feature', 'feature') l 1 s: 990 \n",
      "[==========] 990 990 1.0\n",
      "Validate data  ('question', 'question') l 1 s: 990 \n",
      "[==========] 990 990 1.0\n",
      "accuracy {('bug', 'feature'): 1.0, ('bug', 'question'): 1.0, ('feature', 'question'): 1.0, ('bug', 'bug'): 1.0, ('feature', 'feature'): 1.0, ('question', 'question'): 1.0}\n",
      "new_pos {} \n",
      "\n",
      "{'config': {'save_path': 'output', 'labels': ['bug', 'feature', 'question'], 'epochs': 3, 'validation_data': 0.1, 'chunk_size': 0.25, 'positive_threshold': 0.95, 'negative_threshold': 0.05, 'chunks_reviewed': 3, 'batch_size': 32, 'min_chunk_size': 0.2}, 0: {'bug_feature': 1.0, 'bug_question': 1.0, 'feature_question': 1.0, 'bug_bug': 1.0, 'feature_feature': 1.0, 'question_question': 1.0}}\n",
      "{'config': {'save_path': 'output', 'labels': ['bug', 'feature', 'question'], 'epochs': 3, 'validation_data': 0.1, 'chunk_size': 0.25, 'positive_threshold': 0.95, 'negative_threshold': 0.05, 'chunks_reviewed': 3, 'batch_size': 32, 'min_chunk_size': 0.2}, 0: {'bug_feature': 1.0, 'bug_question': 1.0, 'feature_question': 1.0, 'bug_bug': 1.0, 'feature_feature': 1.0, 'question_question': 1.0}}\n",
      "Iteration 1\n",
      "bug feature st_pos 4500.0 size 4083.3333333333335 end_pos 8583.333333333334\n",
      "bug question st_pos 4500.0 size 4083.3333333333335 end_pos 8583.333333333334\n",
      "feature question st_pos 4500.0 size 4083.3333333333335 end_pos 8583.333333333334\n",
      "bug bug st_pos 4455.0 size 4042.5 end_pos 8497.5\n",
      "feature feature st_pos 4455.0 size 4042.5 end_pos 8497.5\n",
      "question question st_pos 4455.0 size 4042.5 end_pos 8497.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n",
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n",
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n",
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n",
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n",
      "/home/afbenitez/NLSBE/CFFitST/CFFit.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  add[\"score\"] = pair_info[(label_a, label_b)][\"score\"]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m cff_model \u001b[38;5;241m=\u001b[39m CFFitST\u001b[38;5;241m.\u001b[39mfrom_pretrained(BASE_MODEL)\n\u001b[1;32m      8\u001b[0m cff_model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mcff_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbug\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRANDOM_SEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositive_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunks_reviewed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_chunk_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_model_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m cff_head \u001b[38;5;241m=\u001b[39m ClassificationHead(cff_model)\n\u001b[1;32m     16\u001b[0m cff_head\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m~/NLSBE/CFFitST/CFFit.py:239\u001b[0m, in \u001b[0;36mCFFitST.fit\u001b[0;34m(self, df, labels, random_state, column_label, column_sentence, epochs, validation_data, chunk_size, positive_threshold, negative_threshold, chunks_reviewed, batch_size, min_chunk_size, verbose, save_path, name)\u001b[0m\n\u001b[1;32m    236\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m get_dataloader(df_train)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# entrenamiento\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mst_model\u001b[38;5;241m.\u001b[39mfit(train_objectives\u001b[38;5;241m=\u001b[39m[(train_dataloader, train_loss)], \n\u001b[1;32m    240\u001b[0m                    epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m    241\u001b[0m                    show_progress_bar\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# evaluacion de modelo\u001b[39;00m\n\u001b[1;32m    244\u001b[0m dic_accuracy, gen_accuracy, sum_accuracy, dic_traz \u001b[38;5;241m=\u001b[39m validate_data()\n",
      "File \u001b[0;32m~/NLSBE/venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:722\u001b[0m, in \u001b[0;36mSentenceTransformer.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss_model(features, labels)\n\u001b[0;32m--> 722\u001b[0m     \u001b[43mloss_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(loss_model\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)\n\u001b[1;32m    724\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/NLSBE/venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLSBE/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from CFFit import CFFitST, ClassificationHead\n",
    "\n",
    "results = defaultdict(dict)\n",
    "for repo in repos:\n",
    "    train_set_repo, test_set_repo = train_set[train_set[\"repo\"]==repo], test_set[train_set[\"repo\"]==repo]\n",
    "    \n",
    "    cff_model = CFFitST.from_pretrained(BASE_MODEL)\n",
    "    cff_model.to(DEVICE)\n",
    "    cff_model.fit(train_set_repo, [\"bug\",\"feature\",\"question\"], random_state = RANDOM_SEED,\\\n",
    "            epochs=3, validation_data=0.1, chunk_size=0.2,\\\n",
    "            positive_threshold=0.95, negative_threshold=0.05,\\\n",
    "            chunks_reviewed =3, batch_size = 32, min_chunk_size = 0.25, verbose=False,\\\n",
    "            save_path = OUTPUT_PATH, name=repo.replace(\"/\",\"_\")+\"_model_test\")\n",
    "    \n",
    "    cff_head = ClassificationHead(cff_model)\n",
    "    cff_head.to(DEVICE)\n",
    "    x, y = get_x_y(train_set_repo)\n",
    "    cff_head.fit(x,y,epochs=2)\n",
    "    y_pred = cff_head.predict(test_set_repo['text'])\n",
    "    \n",
    "    results[repo]['metrics'] = class_report(get_labels(test_set_repo), y_pred,repo)\n",
    "    results[repo]['predictions'] = y_pred.tolist()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de79e5-54c9-42b4-a47d-4e75d31656b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(results['label_mapping'])\n",
    "for repo in repos:\n",
    "    print(repo)\n",
    "    print(json.dumps(results[repo]['metrics'], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53297178-1345-4b7c-b6c6-cc12fc77662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_metrics_sum = defaultdict(defaultdict)\n",
    "labels = [key for key in results[repos[0]]['metrics'].keys() if key.isnumeric()]\n",
    "\n",
    "for repo in repos:\n",
    "    for label in labels:\n",
    "        for metric in results[repo]['metrics'][label]:\n",
    "            class_metrics_sum[label][metric] = class_metrics_sum[label].get(metric, 0) + results[repo]['metrics'][label][metric]\n",
    "\n",
    "class_metrics_avg = {\n",
    "    label: {\n",
    "        metric: class_metrics_sum[label][metric] / len(repos)\n",
    "        for metric in class_metrics_sum[label]\n",
    "    }\n",
    "    for label in labels\n",
    "}\n",
    "\n",
    "# add the average of the metric over all classes\n",
    "class_metrics_avg['average'] = {\n",
    "    metric: sum(class_metrics_avg[label][metric] for label in labels)\n",
    "    / len(labels)\n",
    "    for metric in class_metrics_avg[labels[0]]\n",
    "}\n",
    "\n",
    "# add to the results    \n",
    "results['overall'] = {\n",
    "    'metrics': class_metrics_avg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beeae4d-f143-4592-8877-d39385a8b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_file_name = 'results.json'\n",
    "with open(os.path.join(OUTPUT_PATH, output_file_name), 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0658299-266a-424e-ac61-b0c30f438c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
