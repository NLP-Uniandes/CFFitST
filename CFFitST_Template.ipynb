{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137491cb-0ac0-4db9-ab2f-f210862d9457",
   "metadata": {},
   "source": [
    "# CFFitST Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a2ca4-36e6-4702-841c-3ae00802be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from CFFit import CFFitST, ClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ffe364-3684-4b13-9a0d-ddef455e78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"all-mpnet-base-v2\" # pretrained sentence transformer model\n",
    "RANDOM_SEED = 23 # random seed\n",
    "OUTPUT_PATH = 'output' # output directory\n",
    "FRACTION = 1 # fraction of training data\n",
    "THRES_POS = 0.95 # minimum cosine similarity to accept enough similarity between embeddings of a positive examples to classify as correct\n",
    "THRES_NEG = 0.05 # maximum cosines similarity to accept enough similarity between embeddings of a negative example to classify as correct\n",
    "LEARNING_RATE = 0.001 # learning rate used in SentenceTransformer fitting\n",
    "\n",
    "import torch\n",
    "# selecting cuda device\n",
    "device_num = 3\n",
    "DEVICE = \"cuda:\"+str(device_num)\n",
    "torch.cuda.set_device(device_num)\n",
    "DEVICE = DEVICE if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a074e-1c17-4ffc-a6a8-2ae991d212a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test and train data\n",
    "train_set = pd.read_csv(\"data/issues_train.csv\")\n",
    "test_set = pd.read_csv(\"data/issues_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fa414-42d2-46f6-825b-279e328e20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = list(set(train_set[\"repo\"].unique()))\n",
    "print(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6e24e-d111-4b96-ba59-cc20e7886d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.groupby([\"repo\", \"label\"]).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90714e6-b008-41e4-bf42-1c325eb32364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# prepare input as the sum of title and body\n",
    "def process_dataset(df):\n",
    "    for i, row in df.iterrows():\n",
    "        #print(row)\n",
    "        df.at[i,'text'] = str(row['title']) + \" \" + str(row['body'])\n",
    "    df = df[['text', 'label', 'repo']]\n",
    "    return df\n",
    "    \n",
    "train_set, test_set = process_dataset(train_set), process_dataset(test_set)\n",
    "\n",
    "# function to return labels\n",
    "dic_labels = {\"feature\":0,\"bug\":1,\"question\":2}\n",
    "def get_labels(data_set):\n",
    "    labels = data_set[\"label\"]\n",
    "    return to_categorical([ dic_labels[label] for i, label in labels.items()], num_classes=3)\n",
    "    \n",
    "# get input and labels from df\n",
    "def get_x_y(df):\n",
    "    x = df[\"text\"].to_list()\n",
    "    y = get_labels(df)\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940ba91-06d3-4038-a90e-d943db32eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to generate a classification report within execution\n",
    "dic_labels = {\"feature\":0,\"bug\":1,\"question\":2}\n",
    "def class_report(y_true, y_pred, name_repo):\n",
    "  \n",
    "  # Convert the predicted probabilities to class labels\n",
    "  y_pred_classes = np.argmax(y_pred, axis=1)  # Assuming a one-hot encoded target variable\n",
    "\n",
    "  # Convert the true labels to class labels (if needed)\n",
    "  y_true_classes = np.argmax(y_true, axis=1)  # Replace 'y_true' with your true labels\n",
    "\n",
    "  # Generate the classification report\n",
    "  report = classification_report(y_true_classes, y_pred_classes)\n",
    "  print(name_repo)\n",
    "  print(report)\n",
    "    \n",
    "  # Make confusion matrix\n",
    "  matrix_confusion = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "  # Heatmap for the confusion matrix\n",
    "  plt.figure(figsize=(4, 3))\n",
    "  sns.heatmap(matrix_confusion, annot=True, fmt='d', cmap='Blues',\n",
    "              xticklabels=list(dic_labels.keys()), yticklabels=list(dic_labels.keys()))\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.ylabel('Actual')\n",
    "  plt.title(name_repo)\n",
    "  plt.show()\n",
    "  return classification_report(y_true_classes, y_pred_classes, output_dict=True, digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179aae76-0d87-4788-82ec-44e4b0973c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from CFFit import CFFitST, ClassificationHead\n",
    "\n",
    "results = defaultdict(dict)\n",
    "for repo in repos:\n",
    "    # get train and test sets filtering rows by repository\n",
    "    train_set_repo, test_set_repo = train_set[train_set[\"repo\"]==repo], test_set[test_set[\"repo\"]==repo]\n",
    "\n",
    "    # initialize CFFitST model\n",
    "    cff_model = CFFitST.from_pretrained(BASE_MODEL)\n",
    "    # set device\n",
    "    cff_model.to(DEVICE)\n",
    "    # training method\n",
    "    cff_model.fit(train_set_repo.sample(frac=FRACTION,random_state=RANDOM_SEED), [\"bug\",\"feature\",\"question\"], random_state = RANDOM_SEED,\\\n",
    "            epochs=3, validation_data=0.1, chunk_size=0.2,\\\n",
    "            positive_threshold=THRES_POS, negative_threshold=THRES_NEG,\\\n",
    "            chunks_reviewed =3, batch_size = 32, min_chunk_size = 0.25, verbose=False,\\\n",
    "            save_path = OUTPUT_PATH, name=repo.replace(\"/\",\"_\")+\"_baseline\"+\"_\"+str(FRACTION))\n",
    "    # initialize classification head\n",
    "    cff_head = ClassificationHead(cff_model)\n",
    "    # set cuda device for classification head (to avoid cuda crashes, select the same cusa device of the CFFitST object)\n",
    "    cff_head.to(DEVICE)\n",
    "    x, y = get_x_y(train_set_repo.sample(frac=FRACTION,random_state=RANDOM_SEED))\n",
    "    cff_head.fit(x,y,epochs=40,learning_rate=LEARNING_RATE)\n",
    "    y_pred = cff_head.predict(test_set_repo['text'])\n",
    "    \n",
    "    results[repo]['metrics'] = class_report(get_labels(test_set_repo), y_pred,repo)\n",
    "    results[repo]['predictions'] = y_pred.tolist()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de79e5-54c9-42b4-a47d-4e75d31656b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(results['label_mapping'])\n",
    "for repo in repos:\n",
    "    print(repo)\n",
    "    print(json.dumps(results[repo]['metrics'], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53297178-1345-4b7c-b6c6-cc12fc77662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_metrics_sum = defaultdict(defaultdict)\n",
    "labels = [key for key in results[repos[0]]['metrics'].keys() if key.isnumeric()]\n",
    "\n",
    "for repo in repos:\n",
    "    for label in labels:\n",
    "        for metric in results[repo]['metrics'][label]:\n",
    "            class_metrics_sum[label][metric] = class_metrics_sum[label].get(metric, 0) + results[repo]['metrics'][label][metric]\n",
    "\n",
    "class_metrics_avg = {\n",
    "    label: {\n",
    "        metric: class_metrics_sum[label][metric] / len(repos)\n",
    "        for metric in class_metrics_sum[label]\n",
    "    }\n",
    "    for label in labels\n",
    "}\n",
    "\n",
    "# add the average of the metric over all classes\n",
    "class_metrics_avg['average'] = {\n",
    "    metric: sum(class_metrics_avg[label][metric] for label in labels)\n",
    "    / len(labels)\n",
    "    for metric in class_metrics_avg[labels[0]]\n",
    "}\n",
    "\n",
    "# add to the results    \n",
    "results['overall'] = {\n",
    "    'metrics': class_metrics_avg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beeae4d-f143-4592-8877-d39385a8b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_file_name = 'results.json'\n",
    "with open(os.path.join(OUTPUT_PATH, output_file_name), 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
